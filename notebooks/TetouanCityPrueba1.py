# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yhqZuLmhl4dKhxBUb5VzqvdiopGls1gn

# Fase 1
## A01736690 - José Ashamat Jaimes Saavedra
### MLOps

# EDA
"""

# ==== 1. Importación de librerías ====
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

from google.colab import drive
drive.mount('/gdrive')

# Configuraciones globales
sns.set(style="whitegrid", palette="pastel")
plt.rcParams['figure.figsize'] = (10,6)

PATH = '/gdrive/MyDrive/Colab Notebooks/MLOps/data/f1/power_tetouan_city_modified.csv'
df = pd.read_csv(PATH)

df.head(10)

df.info()

# ==== 3. Revisión inicial ====
print("Dimensiones del dataset:", df.shape)
print("\nTipos de datos:")
print(df.dtypes)
print("\nValores nulos por columna:\n", df.isnull().sum())
print("\nDuplicados en el dataset:", df.duplicated().sum())

# ==== 4. Conversión de tipos ====
df["DateTime"] = pd.to_datetime(df["DateTime"].astype(str).str.strip(), format='mixed', dayfirst=True, errors='coerce')

# Revisar si hay fechas no convertidas (NaT)
invalid_dates = df["DateTime"].isna().sum()
print(f"Fechas no convertibles: {invalid_dates}")

invalid_df = df[df["DateTime"].isna()]
print("Ejemplos de fechas no convertibles:")
print(invalid_df["DateTime"].head(10))

# Quitar espacios y caracteres no numéricos excepto signo, punto o coma
for col in df.columns:
    if col != "DateTime":  # evitamos tocar la columna de fechas
        df[col] = (
            df[col]
            .astype(str)
            .str.strip()
            .str.replace(",", ".", regex=False)  # cambia coma decimal por punto
            .str.replace(r"[^0-9.\-]", "", regex=True)  # elimina texto o símbolos
        )

# Intentar convertir todas las columnas (excepto DateTime) a numéricas
for col in df.columns:
    if col != "DateTime":
        df[col] = pd.to_numeric(df[col])

print("\nTipos de datos después de la conversión:")
print(df.dtypes)
print("\nValores nulos por columna:")
print(df.isna().sum())

# ==== 5. Estadísticos descriptivos ====
print("\nEstadísticos descriptivos:")
display(df.describe().T)

PATH = '/gdrive/MyDrive/Colab Notebooks/MLOps/data/f1/power_tetouan_city_modified_typed.csv'
df.to_csv(PATH, index=False)

cond_temp = (df['Temperature'] < -10) | (df['Temperature'] > 80)
cond_hum = (df['Humidity'] < 0) | (df['Humidity'] > 100)
cond_wind = (df['Wind Speed'] < 0) | (df['Wind Speed'] > 50)

print("Valores anómalos:")
print("Temperature:", cond_temp.sum())
print("Humidity:", cond_hum.sum())
print("Wind Speed:", cond_wind.sum())

df.loc[cond_temp, 'Temperature'] = np.nan
df.loc[cond_hum, 'Humidity'] = np.nan
df.loc[cond_wind, 'Wind Speed'] = np.nan

zones = ['Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']
print("\nDetección de outliers por IQR en consumo energético:")

for col in zones:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))
    outlier_count = outliers.sum()
    print(f"  {col}: {outlier_count} registros ({outlier_count/len(df)*100:.2f}%) fuera del rango IQR.")

for col in zones:
    upper_limit = df[col].quantile(0.99)
    df.loc[df[col] > upper_limit, col] = upper_limit

print("\nEstadísticos después de limpieza:")
display(df.describe().T)

(df[['Temperature', 'Humidity', 'Wind Speed']] < 0).sum()
(df[['Temperature', 'Humidity']] > 100).sum()



# ==== 6. Distribución de variables numéricas ====
numeric_cols = df.select_dtypes(include=np.number).columns

for col in numeric_cols:
    plt.figure()
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribución de {col}")
    plt.xlabel(col)
    plt.ylabel("Frecuencia")
    plt.show()

# ==== 7. Boxplots para detectar outliers ====
for col in numeric_cols:
    plt.figure()
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot de {col}")
    plt.show()

plt.figure(figsize=(10,4))
sns.heatmap(df.isnull(), cbar=False)
plt.title("Mapa de valores nulos")
plt.show()

print(df.columns)

# ==== 8. Análisis temporal ====
plt.figure(figsize=(14,6))
sns.lineplot(x='DateTime', y='Zone 1 Power Consumption', data=df, label='Zona 1')
sns.lineplot(x='DateTime', y='Zone 2  Power Consumption', data=df, label='Zona 2')
sns.lineplot(x='DateTime', y='Zone 3  Power Consumption', data=df, label='Zona 3')
plt.title('Consumo de Energía por Zona - Tetouan')
plt.xlabel('Tiempo')
plt.ylabel('Consumo de Energía (kW)')
plt.legend()
plt.show()

# ==== 9. Correlaciones ====
plt.figure(figsize=(10,8))
corr = df[numeric_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Mapa de correlaciones entre variables")
plt.show()

# ==== 10. Relaciones bivariadas (ejemplo: temperatura vs consumo) ====

# Relaciones clave con el consumo
features = ["Temperature", "Humidity", "Wind Speed", "general diffuse flows", "diffuse flows"]
targets = ["Zone 1 Power Consumption", "Zone 2  Power Consumption", "Zone 3  Power Consumption"]

for t in targets:
    for f in features:
        plt.figure()
        sns.scatterplot(data=df, x=f, y=t, alpha=0.5)
        plt.title(f"{f} vs {t}")
        plt.show()

# ==== 11. Análisis temporal por hora del día ====
df['hour'] = df['DateTime'].dt.hour
sns.boxplot(x='hour', y='Zone 1 Power Consumption', data=df)
plt.title('Consumo promedio por hora - Zona 1')
plt.show()

df['hour'] = df['DateTime'].dt.hour
sns.boxplot(x='hour', y='Zone 2  Power Consumption', data=df)
plt.title('Consumo promedio por hora - Zona 2')
plt.show()

df['hour'] = df['DateTime'].dt.hour
sns.boxplot(x='hour', y='Zone 3  Power Consumption', data=df)
plt.title('Consumo promedio por hora - Zona 3')
plt.show()

"""## Limpieza de datos"""

# Eliminar duplicados
df = df.drop_duplicates()

# Detección básica de outliers (por z-score)
from scipy import stats

z_scores = np.abs(stats.zscore(df.select_dtypes(include=np.number)))
outliers = (z_scores > 3).sum(axis=0)
print("Número de posibles outliers por columna:\n", outliers)

print("\nValores nulos por columna:\n", df.isnull().sum())
print("\nDuplicados en el dataset:", df.duplicated().sum())

for col in ['Temperature', 'Humidity', 'Wind Speed']:
    df[col] = df[col].interpolate()

df['hour'] = df['DateTime'].dt.hour
for col in ['Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']:
    df[col] = df.groupby('hour')[col].transform(lambda x: x.fillna(x.mean()))

if 'mixed_type_col' in df.columns:
    df.drop(columns=['mixed_type_col'], inplace=True)
    print("Columna 'mixed_type_col' eliminada exitosamente.\n")

df = df.dropna(subset=['DateTime'])
print(f"Registros eliminados por DateTime nulo: {df['DateTime'].isna().sum()}")

print("\nValores nulos por columna después de limpieza final:")
print(df.isna().sum())

df.reset_index(drop=True, inplace=True)

# Convertimos DateTime a índice para permitir interpolación por tiempo
df = df.set_index('DateTime')
df = df.sort_index()  # muy importante asegurar orden temporal

is_night = (df['hour'] >= 19) | (df['hour'] < 5)

# Radiación nocturna → asignar 0
for col in ['general diffuse flows', 'diffuse flows']:
    df.loc[is_night & df[col].isna(), col] = 0

# Radiación diurna → imputación por interpolación temporal
for col in ['general diffuse flows', 'diffuse flows']:
    df[col] = df[col].interpolate(method='time')

# Si queda algún NA residual (raro), forward-fill/backfill
df[['general diffuse flows', 'diffuse flows']] = df[['general diffuse flows', 'diffuse flows']].fillna(method='bfill').fillna(method='ffill')

# 6. Regresar el índice
df = df.reset_index()

PATH = '/gdrive/MyDrive/Colab Notebooks/MLOps/data/f1/power_tetouan_city_modified_clean.csv'
df.to_csv(PATH, index=False)

from sklearn.preprocessing import RobustScaler

# Seleccionamos columnas numéricas relevantes (omitimos DateTime)
num_cols = ['Temperature', 'Humidity', 'Wind Speed',
            'general diffuse flows', 'diffuse flows',
            'Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']

# Inicializamos el escalador robusto
scaler = RobustScaler()

# Aplicamos el escalador y generamos nuevo DataFrame normalizado
df_scaled = df.copy()
df_scaled[num_cols] = scaler.fit_transform(df[num_cols])

# Verificamos los resultados
print("\nResumen estadístico después de la normalización:\n")
display(df_scaled[num_cols].describe().round(3))

PATH = '/gdrive/MyDrive/Colab Notebooks/MLOps/data/f1/power_tetouan_city_modified_scaled.csv'
df_scaled.to_csv(PATH, index=False)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Seleccionamos solo las columnas numéricas relevantes
features = ['Temperature', 'Humidity', 'Wind Speed',
            'general diffuse flows', 'diffuse flows',
            'Zone 1 Power Consumption', 'Zone 2  Power Consumption', 'Zone 3  Power Consumption']

X = df_scaled[features].dropna()

# Creamos el modelo PCA (por defecto todos los componentes)
pca = PCA()
X_pca = pca.fit_transform(X)

# Calculamos la varianza explicada
explained_var = pca.explained_variance_ratio_

# Graficamos la varianza acumulada
plt.figure(figsize=(8,5))
plt.plot(np.cumsum(explained_var), marker='o', linestyle='--', color='b')
plt.title('Varianza Acumulada por Número de Componentes (PCA)')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza Acumulada')
plt.grid(True)
plt.show()

# Visualizamos la contribución de cada variable al primer componente
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(len(features))],
    index=features
)

plt.figure(figsize=(10,6))
sns.heatmap(loadings, annot=True, cmap='RdBu_r', center=0)
plt.title('Contribución de cada variable a los Componentes Principales')
plt.show()

df_pca = pd.DataFrame(X_pca[:, :3], columns=['PC1', 'PC2', 'PC3'])
df_pca['DateTime'] = df_scaled['DateTime'].reset_index(drop=True)

plt.figure(figsize=(8,6))
sns.scatterplot(x='PC1', y='PC2', data=df_pca, alpha=0.3)
plt.title('Proyección PCA (PC1 vs PC2)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.grid(True)
plt.show()

for i, var in enumerate(explained_var):
    print(f"PC{i+1}: {var*100:.2f}% de varianza explicada")
print(f"\nVarianza acumulada total: {np.cumsum(explained_var)[-1]*100:.2f}%")

"""# Modelos"""

from sklearn.model_selection import train_test_split

X = df[['Temperature', 'Humidity', 'Wind Speed', 'general diffuse flows', 'diffuse flows']]
y = df['Zone 1 Power Consumption']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import cross_val_score, KFold
import numpy as np

cv = KFold(n_splits=5, shuffle=True, random_state=42)

# @title Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)

mae = mean_absolute_error(y_test, y_pred_lr)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2 = r2_score(y_test, y_pred_lr)

print("=== Linear REGRESSION ===")
print(f'MAE: {mae}')
print(f'RMSE: {rmse}')
print(f'R2: {r2}')

# @title Ridge
from sklearn.linear_model import RidgeCV

ridge = RidgeCV(alphas=np.logspace(-3, 3, 20), cv=5)
ridge.fit(X_train, y_train)

# Predicción
ridge_pred = ridge.predict(X_test)

# Métricas
ridge_mae = mean_absolute_error(y_test, ridge_pred)
ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))
ridge_r2 = r2_score(y_test, ridge_pred)

print("=== RIDGE REGRESSION ===")
print(f"Mejor alpha: {ridge.alpha_}")
print(f"MAE:  {ridge_mae:.3f}")
print(f"RMSE: {ridge_rmse:.3f}")
print(f"R²:   {ridge_r2:.3f}\n")

# @title Lasso
from sklearn.linear_model import LassoCV

lasso = LassoCV(alphas=np.logspace(-3, 3, 20), cv=5, max_iter=10000)
lasso.fit(X_train, y_train)

# Predicción
lasso_pred = lasso.predict(X_test)

# Métricas
lasso_mae = mean_absolute_error(y_test, lasso_pred)
lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))
lasso_r2 = r2_score(y_test, lasso_pred)

print("=== LASSO REGRESSION ===")
print(f"Mejor alpha: {lasso.alpha_}")
print(f"MAE:  {lasso_mae:.3f}")
print(f"RMSE: {lasso_rmse:.3f}")
print(f"R²:   {lasso_r2:.3f}\n")

# @title Random Forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5]
}

rf = RandomForestRegressor(random_state=42)

grid_rf = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=1
)

grid_rf.fit(X_train, y_train)

# Mejor modelo encontrado
best_rf = grid_rf.best_estimator_

# Predicción
rf_pred = best_rf.predict(X_test)

# Métricas
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print("=== RANDOM FOREST REGRESSOR ===")
print(f"Mejores hiperparámetros: {grid_rf.best_params_}")
print(f"MAE:  {rf_mae:.3f}")
print(f"RMSE: {rf_rmse:.3f}")
print(f"R²:   {rf_r2:.3f}")

print('Done')

